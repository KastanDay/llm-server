# VLLM [(Github)](https://github.com/vllm-project/vllm)

* OpenAI-compatible API server
* Ultra-low-latency & high throughput inference with customized, fused, cuda kernels.
* Concurrency offered by default (fast multi-user inference). It's phenominal.
* Supports most popular text-only LLMs: [Supported Models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)

